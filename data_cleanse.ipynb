{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\neal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\neal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\neal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (1.21.4)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\neal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\neal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: petl in c:\\users\\neal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.7.4)\n"
     ]
    }
   ],
   "source": [
    "# Required package installations.\n",
    "! pip install pandas\n",
    "! pip install petl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports.\n",
    "import pandas as pd\n",
    "import petl as etl\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables containing location of relevant csv and json.\n",
    "globbed_csv_files = glob.glob(\"data/*.csv\")\n",
    "globbed_json_files = glob.glob(\"data/*.json\")\n",
    "\n",
    "# Make headers in branch data consistent.\n",
    "# The structure of the tables are uniform, therefore only heading names need to be adjusted.\n",
    "headers = ['year', 'month', 'day', 'hour', 'product', 'quantity', 'amount_in_gbp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all relevant branch csv files and add a new column called 'branch_name'.\n",
    "# Clean and convert the 'quantity' field from an object to int.\n",
    "csv_branch = [] \n",
    "for i in globbed_csv_files:\n",
    "\n",
    "    csv_data = pd.read_csv(i, header=0, names=headers, low_memory=False)\n",
    "    extracted_filename = os.path.basename(i[15:-4]) # removes '20xx-20xx_' and '.csv' from the filename\n",
    "    removed_characters = extracted_filename.replace('_',' ') # remove '_' between the branch terms\n",
    "    csv_data['branch_name'] = removed_characters # 'branch_name' is consistent with other files\n",
    "\n",
    "    remove_non_value = csv_data['quantity'].replace('-','0') # remove any '-' from quantity field; not possible to have '-' or minus quantity\n",
    "    csv_data['quantity'] = remove_non_value.astype(str).astype(int)\n",
    "     \n",
    "    csv_branch.append(csv_data)\n",
    "    \n",
    "csv_branch_df = pd.concat(csv_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all relevant branch json files and add a new column called 'branch_name'.\n",
    "# Clean and convert the 'quantity' field from an object to int.\n",
    "json_branch = [] \n",
    "for i in globbed_json_files:\n",
    "\n",
    "    json_data = pd.read_json(i)\n",
    "    json_data.set_axis(headers, axis=1, inplace=True) # convert headers and specify axis\n",
    "    extracted_filename = os.path.basename(i[15:-5]) # removes '20xx-20xx_' and '.json' from the filename\n",
    "    remove_characters = extracted_filename.replace('_',' ') # remove '_' between the branch terms\n",
    "    json_data['branch_name'] = remove_characters # 'branch_name' is consistent with other files\n",
    "\n",
    "    remove_non_value = json_data['quantity'].replace('-','0') # remove any '-' from quantity field; not possible to have '-' or minus quantity\n",
    "    json_data['quantity'] = remove_non_value.astype(str).astype(int)\n",
    "\n",
    "\n",
    "    json_branch.append(json_data)\n",
    "json_branch_df = pd.concat(json_branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the csv and json branches.\n",
    "branches = [csv_branch_df, json_branch_df]\n",
    "all_branch_df = pd.concat(branches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the 'products_list.csv'.\n",
    "products_list_df = pd.read_csv('products_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all branches and product data, merging with product header.\n",
    "all_branch_products_df = all_branch_df.merge(products_list_df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read 'branch_list.xlsx' and 'branch_expenses.xlsx'.\n",
    "region_df = pd.read_excel('data/branch_list.xlsx')\n",
    "expenses_df = pd.read_excel('data/branch_expenses.xlsx')\n",
    "\n",
    "# Merge region_df and expenses_df, merging with 'branch_name'.\n",
    "region_and_expenses_df = region_df.merge(expenses_df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all_branch_products_df, region_and_expenses_df, merging with 'branch_name'.\n",
    "all_bra_prod_reg_exp_df = all_branch_products_df.merge(region_and_expenses_df, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total up 'quantity' for 'category' per 'region' / 'county'. \n",
    "grouped_cat = all_bra_prod_reg_exp_df.groupby(['region', 'county', 'category'])['quantity'].sum().sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total up 'quantity' for 'product' per 'region' / 'county'.\n",
    "grouped_prod = all_bra_prod_reg_exp_df.groupby(['region', 'county', 'product'])['quantity'].sum().sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write grouped categories csv file, per region/county.\n",
    "# Comment out once completed if page is required to be rerun.\n",
    "prep_grouped_cat = etl.fromdataframe(grouped_cat)\n",
    "cat_output = etl.tocsv(prep_grouped_cat, 'grouped_categories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write grouped products csv file, per region/county.\n",
    "# Comment out once completed if page is required to be rerun.\n",
    "prep_grouped_prod = etl.fromdataframe(grouped_prod)\n",
    "prod_output = etl.tocsv(prep_grouped_prod, 'grouped_products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neal\\AppData\\Local\\Temp/ipykernel_5188/220770401.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  grouped_branch_perf = all_bra_prod_reg_exp_df.groupby(['region', 'county', 'branch_name', 'year', 'month', 'day', 'hour'])['quantity','amount_in_gbp'].sum().sort_values(by=['quantity','amount_in_gbp'], ascending=False).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Total up quantity and amount_in_gbp per branch over each date and time\n",
    "grouped_branch_perf = all_bra_prod_reg_exp_df.groupby(['region', 'county', 'branch_name', 'year', 'month', 'day', 'hour'])['quantity','amount_in_gbp'].sum().sort_values(by=['quantity','amount_in_gbp'], ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write grouped branch performance csv file, per region/county.\n",
    "# This file encompases both performance and hours at generic level, ready for filtering down for sales per branch per hour.\n",
    "# Comment out once completed if page is required to be rerun.\n",
    "prep_branch_perf = etl.fromdataframe(grouped_branch_perf)\n",
    "branch_perf_output = etl.tocsv(prep_branch_perf, 'grouped_branch_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance column in branch performance data.\n",
    "columns_list = ['quantity', 'amount_in_gbp']\n",
    "grouped_branch_perf['performance'] = grouped_branch_perf[columns_list].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total up overall performance of each branch per region and county.\n",
    "overall_branch_perf = grouped_branch_perf.groupby(['region', 'county', 'branch_name'])['performance'].sum().sort_values(ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write overall branch performance csv file, per region/county.\n",
    "# Comment out once completed if page is required to be rerun.\n",
    "prep_overall_branch_perf = etl.fromdataframe(overall_branch_perf)\n",
    "branch_perf_output = etl.tocsv(prep_overall_branch_perf, 'overall_branch_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neal\\AppData\\Local\\Temp/ipykernel_5188/2842588614.py:2: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  branch_sales_expenses = all_bra_prod_reg_exp_df.groupby('branch_name')['amount_in_gbp', 'operational_cost', 'staff_bonuses', 'misc_expenses', 'waste_cost'].sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Sum expenses and sales and group by branch_name\n",
    "branch_sales_expenses = all_bra_prod_reg_exp_df.groupby('branch_name')['amount_in_gbp', 'operational_cost', 'staff_bonuses', 'misc_expenses', 'waste_cost'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create total expenses column\n",
    "expenses_list = ['operational_cost', 'staff_bonuses', 'misc_expenses', 'waste_cost']\n",
    "branch_sales_expenses['total_expenses'] = branch_sales_expenses[expenses_list].sum(axis=1)\n",
    "\n",
    "# Create profit column \n",
    "branch_sales_expenses['profit'] = branch_sales_expenses['amount_in_gbp'] - branch_sales_expenses['total_expenses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values by profit (max values at the top)\n",
    "profit_per_branch = branch_sales_expenses.sort_values(by='profit', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write profitability csv file, per branch_name.\n",
    "# Comment out once completed if page is required to be rerun.\n",
    "prep_profit_per_branch = etl.fromdataframe(profit_per_branch)\n",
    "profit_per_branch_output = etl.tocsv(prep_profit_per_branch, 'profit_per_branch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group regions to aid with dash region selection.\n",
    "remove_duplicates = overall_branch_perf.drop_duplicates(subset=['region'])\n",
    "region_list = remove_duplicates['region'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write region list csv file.\n",
    "# Comment out once completed if page is required to be rerun.\n",
    "prep_region_list = etl.fromdataframe(region_list)\n",
    "region_list_output = etl.tocsv(prep_region_list, 'region_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group branches to aid with dash branch selection.\n",
    "remove_duplicates = region_df.drop_duplicates(subset=['branch_name']) \n",
    "branch_list = remove_duplicates['branch_name'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write branch list csv file.\n",
    "# Comment out once completed if page is required to be rerun.\n",
    "prep_branch_name_list = etl.fromdataframe(branch_list)\n",
    "branch_name_list_output = etl.tocsv(prep_branch_name_list, 'branch_name_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script Ends"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e84e7bcc6f739418562f1bda649658d9ded1e22fc656c6fae3671d5f2d3bcb03"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
